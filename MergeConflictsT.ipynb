# 🧩 Merge Conflict Prediction - TensorFlow Implementation

This notebook implements a TensorFlow-based neural network classifier to predict merge conflicts in software projects based on concurrent commits. The goal is to maximize precision to minimize false positives when predicting conflicts.
# 🔍 Setup and Data Loading
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (precision_score, recall_score, f1_score, 
                            confusion_matrix, classification_report, 
                            precision_recall_curve, average_precision_score)
from imblearn.over_sampling import SMOTE
from sklearn.feature_selection import SelectKBest, f_classif

np.random.seed(42)
tf.random.set_seed(42)

sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['axes.labelsize'] = 14
print("🔄 Loading dataset...")
df = pd.read_csv('MergeConflictsDataset.csv', sep=';')
display(df.head())
df.info()

print("\n📊 Class distribution (conflict vs. no conflict):")
conflict_counts = df['conflict'].value_counts()
print(conflict_counts)
print(f"Conflict rate: {conflict_counts[1] / len(df):.2%}")
The severe imbalance means a naive model could achieve 94.56% accuracy by always predicting "no conflict" - but this would be useless for actually detecting conflicts.
# 🧹 Data Preprocessing
non_feature_cols = ['commit', 'parent1', 'parent2', 'ancestor']
feature_cols = [col for col in df.columns if col not in non_feature_cols and col != 'conflict']

X = df[feature_cols]
y = df['conflict']

missing_values = df.isnull().sum()
if missing_values.sum() > 0:
    print("\nMissing values per column:")
    print(missing_values[missing_values > 0])
    df.dropna(inplace=True) 
    print("Dropped rows with missing values.")
else:
    print("\n ✅ No missing values in the dataset.")
## 🔀 Train/Test Split with Stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nTraining set shape: {X_train.shape}, Test set shape: {X_test.shape}")
print(f"Training set conflict ratio: {100 * y_train.mean():.2f}%")
print(f"Test set conflict ratio: {100 * y_test.mean():.2f}%")

## 🔢 Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  
X_test_scaled = scaler.transform(X_test) 

X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

print("\n📐 Feature ranges after scaling (training data):")
for feature in ['added lines', 'deleted lines', 'time', 'nr files', 'modified files']:
    print(f"{feature}: Min={X_train_scaled[feature].min():.2f}, Max={X_train_scaled[feature].max():.2f}, Mean={X_train_scaled[feature].mean():.2f}, StdDev={X_train_scaled[feature].std():.2f}")

## 🔎 Feature Selection 
selector = SelectKBest(f_classif, k=10)
X_train_selected = selector.fit_transform(X_train_scaled, y_train)
X_test_selected = selector.transform(X_test_scaled)

selected_features = [feature_cols[i] for i in selector.get_support(indices=True)]
print(f"\n🔍 Selected top 10 features:\n{selected_features}")
## 🔄 SMOTE for Class Balance
 
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_selected, y_train)

print(f"\n🔄 After SMOTE resampling:")
print(f"Training set shape: {X_train_resampled.shape}")
print(f"Class distribution: {pd.Series(y_train_resampled).value_counts().to_dict()}")
# 🤖 Model Architecture
- We use a **multi-layer neural network with ReLU (Rectified Linear Unit - it only keeps positive contributions to the decision) activations** to model non-linear relationships between features. 
- **Dropout layers** are included to **reduce overfitting** by randomly disabling neurons during training.
- **Batch normalization** helps **stabilize learning** by normalizing the layer outputs and speeds up training.
- The final layer uses a **sigmoid activation**, outputting probabilities from 0 to 1 for binary classification (conflict vs. no conflict).
- The **Adam Optimizer** decides how to update those 12,289 parameters when the network makes mistakes
def build_precision_model(input_shape):
    """Build a neural network optimized for precision"""
    model = keras.Sequential([
        layers.Dense(128, activation='relu', input_shape=(input_shape,)),
        layers.Dropout(0.4),
        layers.BatchNormalization(), # ensures no single feature dominates
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy', # measures how wrong the network's predictions are
        metrics=[
            'accuracy',
            keras.metrics.Precision(name='precision'),
            keras.metrics.Recall(name='recall'),
            keras.metrics.AUC(name='pr_auc', curve='PR') 
        ]
    )
    
    return model

model = build_precision_model(X_train_resampled.shape[1])
print("\n🏗️ Model Architecture:")
model.summary()
- **Param #**: Number of connections (weights) that layer learns
- **Output Shape**: How many "opinions" that layer produces
- **None**: Means "any batch size" - we can process 1 merge or 1000 merges at once

- **Total params: 12,289** - that means our network learns 12,289 different number values that determine how it makes decisions
- **Trainable params: 12,033** - these are parameters that will be updated during training
- **Non-trainable params: 256** - these are parameters are frozen and won't change during training

# 🏋️ Model Training

- **EarlyStopping** stops training if performance on the validation set (PR-AUC) stops improving, preventing overfitting (memorizing training examples instead of learning real patterns).
- **ReduceLROnPlateau** dynamically lowers the learning rate if the model hits a plateau (a phase during training where the model's performance stops improving significantly and instead flattens out), allowing it to fine-tune better.
- We compute class weights to balance the classes during training, because even after SMOTE balancing, class imbalance can still influence training due to potential sampling artifacts.
- We train the model on the resampled data for up to 150 epochs. 20% of the training data is held out for validation to monitor generalization performance.
- Training stops early if validation PR-AUC stops improving.
early_stopping = callbacks.EarlyStopping(
    monitor='val_pr_auc',
    patience=20, # stop training if no improvement in validation PR AUC for 20 epochs
    restore_best_weights=True,
    mode='max'
)

reduce_lr = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6
)

total = len(y_train_resampled)
weight_for_0 = (1 / (total - sum(y_train_resampled))) * (total / 2.0)
weight_for_1 = (1 / sum(y_train_resampled)) * (total / 2.0)
class_weight = {0: weight_for_0, 1: weight_for_1} 

print("\n⚖️ Class weights:", class_weight)

history = model.fit(
    X_train_resampled,
    y_train_resampled,
    epochs=150, # train for a maximum of 150 epochs
    batch_size=64, # look at 64 samples at a time before updating weights
    validation_split=0.2, # use 20% of training data for validation
    class_weight=class_weight,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

plt.figure(figsize=(15, 5))

metrics = ['loss', 'precision', 'recall', 'pr_auc']
for i, metric in enumerate(metrics):
    plt.subplot(1, 4, i+1)
    plt.plot(history.history[metric], label=f'Train {metric}')
    plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')
    plt.title(metric.upper())
    plt.xlabel('Epoch')
    plt.legend()
    
plt.tight_layout()
plt.show()
- Lower loss, fewer mistakes on a test. The training and validation loss go down together, so the model is not overfitting. That means the model is learning and generalizing well.
- In the first 20 epochs, the network is learning the data, and the loss is decreasing. This is a good sign. 
- The training precision improves gradually (normal learning behavior), while the validation precision is stable (no overfitting) and slightly higher than the training precision (good generalization, very few false positives).
- From the Precision-Recall AUC plot we can see that the model performs well in balancing precision and recall.
# 🎯 Precision Optimization

def find_optimal_threshold(model, X, y_true, min_recall=0.3):
    """Find threshold that maximizes precision while maintaining minimum recall"""
    y_proba = model.predict(X, verbose=0).flatten()
    
    thresholds = np.linspace(0.1, 0.9, 50) # test 50 thresholds from 0.1 to 0.9
    results = []
    
    for thresh in thresholds:
        y_pred = (y_proba >= thresh).astype(int)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        
        if recall >= min_recall:
            results.append({
                'threshold': thresh,
                'precision': precision,
                'recall': recall,
                'f1': f1_score(y_true, y_pred)
            })
    
    if not results:
        return 0.5
    
    results_df = pd.DataFrame(results)
    optimal = results_df.loc[results_df['precision'].idxmax()]
    
    return optimal['threshold'], results_df

optimal_threshold, threshold_results = find_optimal_threshold(
    model, X_train_resampled, y_train_resampled
)

print(f"\n🎯 Optimal Threshold: {optimal_threshold:.3f}")

def plot_metrics_vs_thresholds(model, X, y_true):
    y_proba = model.predict(X, verbose=0).flatten()
    thresholds = np.linspace(0.1, 0.9, 50)
    
    metrics = {
        'precision': [],
        'recall': [],
        'f1': [],
    }
    
    for thresh in thresholds:
        y_pred = (y_proba >= thresh).astype(int)
        metrics['precision'].append(precision_score(y_true, y_pred, zero_division=0))
        metrics['recall'].append(recall_score(y_true, y_pred, zero_division=0))
        metrics['f1'].append(f1_score(y_true, y_pred, zero_division=0))
    
    plt.figure(figsize=(14, 6))
    for metric, values in metrics.items():
        plt.plot(thresholds, values, label=metric, linewidth=2)
    
    plt.axvline(x=0.5, color='gray', linestyle='--', label='Default Threshold (0.5)')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title('Model Metrics Across Thresholds')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

plot_metrics_vs_thresholds(model, X_test_selected, y_test)
- For low thresholds, the network predicts conflicts very easily (many false alarms, low precision), but catches almost all real conflicts (high recall).
- For high thresholds, the network predicts conflicts very rarely (many missed conflicts, low recall), but is very precise (few false alarms).
# 📊 Model Evaluation

Test the model with default settings (50% threshold - predict conflict if confidence is above 50%) and optimized settings (predict conflict if confidence is above 75%). 
def evaluate_model(model, X, y_true, threshold=0.5):
    """Comprehensive model evaluation"""
    y_proba = model.predict(X, verbose=0).flatten()
    y_pred = (y_proba >= threshold).astype(int)
    
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    cm = confusion_matrix(y_true, y_pred)
    report = classification_report(y_true, y_pred)
    
    tn, fp, fn, tp = cm.ravel()
    false_alarm_rate = fp / (fp + tn)
    detection_rate = tp / (tp + fn)
    
    precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_proba)
    pr_auc = average_precision_score(y_true, y_proba)
    
    return {
        'metrics': {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'pr_auc': pr_auc
        },
        'business': {
            'false_alarm_rate': false_alarm_rate,
            'detection_rate': detection_rate,
            'true_positives': tp,
            'false_positives': fp,
            'true_negatives': tn,
            'false_negatives': fn
        },
        'confusion_matrix': cm,
        'classification_report': report,
        'precision_curve': precision_curve,
        'recall_curve': recall_curve
    }

print("\n🔍 Evaluation with Default Threshold (0.5):")
default_eval = evaluate_model(model, X_test_selected, y_test)
print(default_eval['classification_report'])

print(f"\n🎯 Evaluation with Optimal Threshold ({optimal_threshold:.3f}):")
optimal_eval = evaluate_model(model, X_test_selected, y_test, optimal_threshold)
print(optimal_eval['classification_report'])

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

sns.heatmap(default_eval['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax1)
ax1.set_title(f'Default Threshold (0.5)\nPrecision: {default_eval["metrics"]["precision"]:.3f}')

sns.heatmap(optimal_eval['confusion_matrix'], annot=True, fmt='d', cmap='Greens', ax=ax2)
ax2.set_title(f'Optimal Threshold ({optimal_threshold:.3f})\nPrecision: {optimal_eval["metrics"]["precision"]:.3f}')

plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(optimal_eval['recall_curve'], optimal_eval['precision_curve'], label=f'PR Curve (AUC = {optimal_eval["metrics"]["pr_auc"]:.3f})')
plt.scatter(optimal_eval['metrics']['recall'], optimal_eval['metrics']['precision'], 
            color='red', s=100, label=f'Optimal Threshold ({optimal_threshold:.3f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
# 🏆 Final Results and Impact

print("\n📈 Performance Metrics:")
print(f"Default Threshold (0.5):")
print(f"  Precision: {default_eval['metrics']['precision']:.4f}")
print(f"  Recall:    {default_eval['metrics']['recall']:.4f}")
print(f"  F1-Score:  {default_eval['metrics']['f1']:.4f}")

print(f"\nOptimal Threshold ({optimal_threshold:.3f}):")
print(f"  Precision: {optimal_eval['metrics']['precision']:.4f} (+{(optimal_eval['metrics']['precision'] - default_eval['metrics']['precision']):.4f})")
print(f"  Recall:    {optimal_eval['metrics']['recall']:.4f} ({(optimal_eval['metrics']['recall'] - default_eval['metrics']['recall']):.4f})")
print(f"  F1-Score:  {optimal_eval['metrics']['f1']:.4f}")

print("\n💼 Impact Analysis:")
print(f"✅ True Conflicts Detected: {optimal_eval['business']['true_positives']}")
print(f"❌ False Alarms: {optimal_eval['business']['false_positives']}")
print(f"⚠️ Missed Conflicts: {optimal_eval['business']['false_negatives']}")
print(f"🎯 False Alarm Rate: {optimal_eval['business']['false_alarm_rate']:.2%}")
print(f"🔍 Conflict Detection Rate: {optimal_eval['business']['detection_rate']:.2%}")
